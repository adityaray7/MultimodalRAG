{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8902768,"sourceType":"datasetVersion","datasetId":5351819},{"sourceId":8935052,"sourceType":"datasetVersion","datasetId":5367200}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Necessary Dependancies","metadata":{}},{"cell_type":"code","source":"!pip install llama-index\n!pip install llama-index-core\n!pip install llama-index-llms-anthropic llama-index-multi-modal-llms-anthropic\n!pip install llama-index-embeddings-huggingface\n!pip install llama-parse\n!pip install bitsandbytes\n!pip install accelerate faiss-cpu pdfplumber\n!pip install langchain langchain_openai langchain_chroma langchain_community\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:22:44.312367Z","iopub.execute_input":"2024-07-12T02:22:44.313438Z","iopub.status.idle":"2024-07-12T02:25:12.824667Z","shell.execute_reply.started":"2024-07-12T02:22:44.313394Z","shell.execute_reply":"2024-07-12T02:25:12.823758Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting llama-index\n  Downloading llama_index-0.10.55-py3-none-any.whl.metadata (11 kB)\nCollecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n  Downloading llama_index_agent_openai-0.2.8-py3-none-any.whl.metadata (729 bytes)\nCollecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\nCollecting llama-index-core==0.10.55 (from llama-index)\n  Downloading llama_index_core-0.10.55-py3-none-any.whl.metadata (2.4 kB)\nCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\nCollecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\n  Downloading llama_index_indices_managed_llama_cloud-0.2.5-py3-none-any.whl.metadata (3.8 kB)\nCollecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\nCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n  Downloading llama_index_llms_openai-0.1.25-py3-none-any.whl.metadata (610 bytes)\nCollecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n  Downloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\nCollecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\nCollecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n  Downloading llama_index_readers_file-0.1.30-py3-none-any.whl.metadata (5.4 kB)\nCollecting llama-index-readers-llama-parse>=0.1.2 (from llama-index)\n  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (6.0.1)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.55->llama-index) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (3.9.1)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (0.6.6)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (1.2.14)\nCollecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.55->llama-index)\n  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (2024.3.1)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (1.5.8)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (3.2.1)\nCollecting nltk<4.0.0,>=3.8.1 (from llama-index-core==0.10.55->llama-index)\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (1.26.4)\nCollecting openai>=1.1.0 (from llama-index-core==0.10.55->llama-index)\n  Downloading openai-1.35.13-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (2.2.1)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (9.5.0)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (8.2.3)\nCollecting tiktoken>=0.3.3 (from llama-index-core==0.10.55->llama-index)\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (4.9.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core==0.10.55->llama-index) (1.14.1)\nCollecting llama-cloud>=0.0.9 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index)\n  Downloading llama_cloud-0.0.9-py3-none-any.whl.metadata (750 bytes)\nCollecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\nCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\nCollecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index)\n  Downloading llama_parse-0.4.6-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.55->llama-index) (4.0.3)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\nRequirement already satisfied: pydantic>=1.10 in /opt/conda/lib/python3.10/site-packages (from llama-cloud>=0.0.9->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.5.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.55->llama-index) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.55->llama-index) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.55->llama-index) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.55->llama-index) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.55->llama-index) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.55->llama-index) (0.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama-index) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama-index) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.55->llama-index) (2023.12.25)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core==0.10.55->llama-index) (1.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.55->llama-index) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.55->llama-index) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.55->llama-index) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.55->llama-index) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core==0.10.55->llama-index) (3.21.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.55->llama-index) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.55->llama-index) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.55->llama-index) (2023.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core==0.10.55->llama-index) (1.2.0)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.55->llama-index) (21.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.10->llama-cloud>=0.0.9->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.10->llama-cloud>=0.0.9->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.14.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.55->llama-index) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.55->llama-index) (3.1.1)\nDownloading llama_index-0.10.55-py3-none-any.whl (6.8 kB)\nDownloading llama_index_core-0.10.55-py3-none-any.whl (15.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading llama_index_agent_openai-0.2.8-py3-none-any.whl (13 kB)\nDownloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\nDownloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\nDownloading llama_index_indices_managed_llama_cloud-0.2.5-py3-none-any.whl (9.3 kB)\nDownloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\nDownloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\nDownloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\nDownloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\nDownloading llama_index_readers_file-0.1.30-py3-none-any.whl (38 kB)\nDownloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\nDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\nDownloading llama_cloud-0.0.9-py3-none-any.whl (146 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_parse-0.4.6-py3-none-any.whl (9.1 kB)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.35.13-py3-none-any.whl (328 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\nDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: striprtf, dirtyjson, nltk, beautifulsoup4, tiktoken, openai, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: beautifulsoup4\n    Found existing installation: beautifulsoup4 4.12.2\n    Uninstalling beautifulsoup4-4.12.2:\n      Successfully uninstalled beautifulsoup4-4.12.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed beautifulsoup4-4.12.3 dirtyjson-1.0.8 llama-cloud-0.0.9 llama-index-0.10.55 llama-index-agent-openai-0.2.8 llama-index-cli-0.1.12 llama-index-core-0.10.55 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.5 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.25 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.30 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.6 nltk-3.8.1 openai-1.35.13 striprtf-0.0.26 tiktoken-0.7.0\nRequirement already satisfied: llama-index-core in /opt/conda/lib/python3.10/site-packages (0.10.55)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (6.0.1)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (3.9.1)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (0.6.6)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.0.8)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (2024.3.1)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.5.8)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (3.2.1)\nRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (3.8.1)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.26.4)\nRequirement already satisfied: openai>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.35.13)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (2.2.1)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (9.5.0)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (8.2.3)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (0.7.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (4.9.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.14.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (2023.12.25)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core) (1.9.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core) (1.3.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core) (3.6)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core) (3.21.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core) (2023.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core) (1.2.0)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (21.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core) (2.14.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (3.1.1)\nCollecting llama-index-llms-anthropic\n  Downloading llama_index_llms_anthropic-0.1.15-py3-none-any.whl.metadata (690 bytes)\nCollecting llama-index-multi-modal-llms-anthropic\n  Downloading llama_index_multi_modal_llms_anthropic-0.1.4-py3-none-any.whl.metadata (674 bytes)\nCollecting anthropic<0.29.0,>=0.26.2 (from llama-index-llms-anthropic)\n  Downloading anthropic-0.28.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-llms-anthropic) (0.10.55)\nINFO: pip is looking at multiple versions of llama-index-multi-modal-llms-anthropic to determine which version is compatible with other requirements. This could take a while.\nCollecting llama-index-multi-modal-llms-anthropic\n  Downloading llama_index_multi_modal_llms_anthropic-0.1.3-py3-none-any.whl.metadata (666 bytes)\n  Downloading llama_index_multi_modal_llms_anthropic-0.1.2-py3-none-any.whl.metadata (666 bytes)\n  Downloading llama_index_multi_modal_llms_anthropic-0.1.1-py3-none-any.whl.metadata (717 bytes)\n  Downloading llama_index_multi_modal_llms_anthropic-0.1.0-py3-none-any.whl.metadata (666 bytes)\nCollecting llama-index-llms-anthropic\n  Downloading llama_index_llms_anthropic-0.1.14-py3-none-any.whl.metadata (690 bytes)\nINFO: pip is still looking at multiple versions of llama-index-multi-modal-llms-anthropic to determine which version is compatible with other requirements. This could take a while.\n  Downloading llama_index_llms_anthropic-0.1.13-py3-none-any.whl.metadata (639 bytes)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading llama_index_llms_anthropic-0.1.12-py3-none-any.whl.metadata (639 bytes)\n  Downloading llama_index_llms_anthropic-0.1.11-py3-none-any.whl.metadata (639 bytes)\nCollecting anthropic<0.24.0,>=0.23.1 (from llama-index-llms-anthropic)\n  Downloading anthropic-0.23.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (1.3.0)\nRequirement already satisfied: tokenizers>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (0.19.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (4.9.0)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (6.0.1)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (3.9.1)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (0.6.6)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.0.8)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (2024.3.1)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.5.8)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (3.2.1)\nRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (3.8.1)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.26.4)\nRequirement already satisfied: openai>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.35.13)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (2.2.1)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (9.5.0)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (8.2.3)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (0.7.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (4.66.4)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.14.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (4.0.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (0.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (2023.12.25)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (3.0.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (0.23.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (3.21.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (3.13.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (21.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-anthropic) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.24.0,>=0.23.1->llama-index-llms-anthropic) (3.1.1)\nDownloading llama_index_llms_anthropic-0.1.11-py3-none-any.whl (6.1 kB)\nDownloading llama_index_multi_modal_llms_anthropic-0.1.4-py3-none-any.whl (5.8 kB)\nDownloading anthropic-0.23.1-py3-none-any.whl (869 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.1/869.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: anthropic, llama-index-multi-modal-llms-anthropic, llama-index-llms-anthropic\nSuccessfully installed anthropic-0.23.1 llama-index-llms-anthropic-0.1.11 llama-index-multi-modal-llms-anthropic-0.1.4\nCollecting llama-index-embeddings-huggingface\n  Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl.metadata (769 bytes)\nRequirement already satisfied: huggingface-hub>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.23.2)\nRequirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-embeddings-huggingface) (0.10.55)\nCollecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.9.1)\nCollecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.25)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.6)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.8)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.5.8)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.2.1)\nRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.8.1)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.26.4)\nRequirement already satisfied: openai>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.35.13)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.2.1)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (9.5.0)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.2.3)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.14.1)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.41.2)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.1.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.11.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2023.12.25)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.9.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.3.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.6)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.14.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.12.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.2)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.21.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2023.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.2.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.14.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\nDownloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl (7.2 kB)\nDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: minijinja, sentence-transformers, llama-index-embeddings-huggingface\nSuccessfully installed llama-index-embeddings-huggingface-0.2.2 minijinja-2.0.1 sentence-transformers-3.0.1\nRequirement already satisfied: llama-parse in /opt/conda/lib/python3.10/site-packages (0.4.6)\nRequirement already satisfied: llama-index-core>=0.10.29 in /opt/conda/lib/python3.10/site-packages (from llama-parse) (0.10.55)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (6.0.1)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.10.29->llama-parse) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (3.9.1)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (0.6.6)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.0.8)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (2024.3.1)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.5.8)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (3.2.1)\nRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (3.8.1)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.26.4)\nRequirement already satisfied: openai>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.35.13)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (2.2.1)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (9.5.0)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (8.2.3)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (0.7.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (4.9.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core>=0.10.29->llama-parse) (1.14.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.10.29->llama-parse) (4.0.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core>=0.10.29->llama-parse) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core>=0.10.29->llama-parse) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core>=0.10.29->llama-parse) (2023.12.25)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (1.9.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (1.3.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core>=0.10.29->llama-parse) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core>=0.10.29->llama-parse) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core>=0.10.29->llama-parse) (3.6)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core>=0.10.29->llama-parse) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core>=0.10.29->llama-parse) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core>=0.10.29->llama-parse) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.10.29->llama-parse) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core>=0.10.29->llama-parse) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core>=0.10.29->llama-parse) (3.21.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core>=0.10.29->llama-parse) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core>=0.10.29->llama-parse) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core>=0.10.29->llama-parse) (2023.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (1.2.0)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.10.29->llama-parse) (21.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core>=0.10.29->llama-parse) (2.14.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core>=0.10.29->llama-parse) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.10.29->llama-parse) (3.1.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nCollecting pdfplumber\n  Downloading pdfplumber-0.11.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nCollecting pdfminer.six==20231228 (from pdfplumber)\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from pdfplumber) (9.5.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (41.0.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\nDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pdfplumber-0.11.2-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdfium2, faiss-cpu, pdfminer.six, pdfplumber\nSuccessfully installed faiss-cpu-1.8.0.post1 pdfminer.six-20231228 pdfplumber-0.11.2 pypdfium2-4.30.0\nCollecting langchain\n  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\nCollecting langchain_openai\n  Downloading langchain_openai-0.1.15-py3-none-any.whl.metadata (2.5 kB)\nCollecting langchain_chroma\n  Downloading langchain_chroma-0.1.2-py3-none-any.whl.metadata (1.3 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.12 (from langchain)\n  Downloading langchain_core-0.2.16-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.85-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: openai<2.0.0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from langchain_openai) (1.35.13)\nRequirement already satisfied: tiktoken<1,>=0.7 in /opt/conda/lib/python3.10/site-packages (from langchain_openai) (0.7.0)\nCollecting chromadb<0.6.0,>=0.4.0 (from langchain_chroma)\n  Downloading chromadb-0.5.4-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: fastapi<1,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from langchain_chroma) (0.108.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.6)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nCollecting build>=1.0.3 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting chroma-hnswlib==0.7.5 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading chroma_hnswlib-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.25.0)\nCollecting posthog>=2.4.0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.9.0)\nCollecting onnxruntime>=1.14.1 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.19.1)\nCollecting pypika>=0.48.9 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.66.4)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (6.1.1)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.59.3)\nCollecting bcrypt>=4.0.1 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting mmh3>=4.0.1 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting orjson>=3.9.12 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m79.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.27.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.32.0.post1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.12->langchain)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.9.0)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.2.0)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.0.1)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain) (2.4)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.12.1)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.14.1)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.23.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (8.1.7)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (12.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2024.3.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.17.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.5.1)\nDownloading langchain-0.2.7-py3-none-any.whl (983 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.1.15-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_chroma-0.1.2-py3-none-any.whl (9.3 kB)\nDownloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chromadb-0.5.4-py3-none-any.whl (581 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.4/581.4 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.16-py3-none-any.whl (362 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.4/362.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.85-py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\nDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=2724fb195c471865498c2a642b5e704f40480d67419056fc7998d1382331a2bb\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, mmh3, pyproject_hooks, packaging, orjson, opentelemetry-util-http, humanfriendly, chroma-hnswlib, bcrypt, asgiref, posthog, coloredlogs, build, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, opentelemetry-instrumentation-asgi, langchain-core, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain_openai, langchain, chromadb, langchain_community, langchain_chroma\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 bcrypt-4.1.3 build-1.2.1 chroma-hnswlib-0.7.5 chromadb-0.5.4 coloredlogs-15.0.1 humanfriendly-10.0 kubernetes-30.1.0 langchain-0.2.7 langchain-core-0.2.16 langchain-text-splitters-0.2.2 langchain_chroma-0.1.2 langchain_community-0.2.7 langchain_openai-0.1.15 langsmith-0.1.85 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.18.1 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.10.6 packaging-24.1 posthog-3.5.0 pypika-0.48.9 pyproject_hooks-1.1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Architecture","metadata":{}},{"cell_type":"markdown","source":"1. Init apis","metadata":{}},{"cell_type":"code","source":"import nest_asyncio\nnest_asyncio.apply()\nfrom openai import AzureOpenAI\nimport os\nimport numpy as np\nfrom pathlib import Path\nfrom llama_index.readers.file import PDFReader\nimport json\nfrom langchain_openai import AzureOpenAIEmbeddings,AzureChatOpenAI\nimport pickle\n\n# API access to llama-cloud\nos.environ[\"LLAMA_CLOUD_API_KEY\"] = \"\"\n\n# Using Anthropic API for embeddings/LLMs\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\n\nllm = AzureChatOpenAI(\n    azure_deployment=\"\",\n    api_version=\"\",\n    temperature=0\n)\n\nembeddings_model = AzureOpenAIEmbeddings(\n    model = '',\n    openai_api_version=\"\",\n)\n\ndef get_query(messages):\n    response = llm.invoke(messages)\n    return response.content\n\ndef get_embedding(text, model=\"text-embedding3\"):\n   text = text.replace(\"\\n\", \" \")\n   return embeddings_model.embed_query(text)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:25:12.826602Z","iopub.execute_input":"2024-07-12T02:25:12.826911Z","iopub.status.idle":"2024-07-12T02:25:16.746088Z","shell.execute_reply.started":"2024-07-12T02:25:12.826883Z","shell.execute_reply":"2024-07-12T02:25:16.745298Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"2. Vision Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,AutoModel\n\ndevice = \"cuda\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4v-9b\", trust_remote_code=True)\n\n\nmodel = AutoModel.from_pretrained(\n    \"THUDM/glm-4v-9b\",\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).eval()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:47:47.724282Z","iopub.execute_input":"2024-07-12T03:47:47.724760Z","iopub.status.idle":"2024-07-12T04:17:05.343386Z","shell.execute_reply.started":"2024-07-12T03:47:47.724729Z","shell.execute_reply":"2024-07-12T04:17:05.342534Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":104,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1950337418a4f9d9e79e7c342a1bb75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_chatglm.py:   0%|          | 0.00/17.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80c0610ca9e44fceade3c7addaf5f363"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/glm-4v-9b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/2.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff994066d744b75af1c3804791ebe6b"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd4f9349036449594b8d2a402c51835"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_chatglm.py:   0%|          | 0.00/2.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2864cf7214d3473486bb445bd8cf7541"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/glm-4v-9b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_chatglm.py:   0%|          | 0.00/68.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d981caeb3f849698df85a8a1b5ac0d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"visual.py:   0%|          | 0.00/6.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f8cbc7ea214e9c95da78807f99aa34"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/glm-4v-9b:\n- visual.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/glm-4v-9b:\n- modeling_chatglm.py\n- visual.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/111k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1096876a0584051bb4f852b176f8b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4511944dbf944e2ba3b1e4348775e86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00015.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa03cb682634f1ca457443f9ab111ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00015.safetensors:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed149eb36894dbe82670e01edf7b981"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00015.safetensors:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bf129923a664408afe16bee447bc401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00015.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47c32b5196b94448b862eecacb965be2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00015.safetensors:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa91ae9dedf743c780c15865ad7628bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00015.safetensors:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce30faff06b4bdf92fc6fe1eda1c592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00015.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c5b069ef7a9474eaa2e0138d9ddae21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00015.safetensors:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e186853ef0524aea8dc47f34937609ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00009-of-00015.safetensors:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d43564160a26416b977d5505a1deeccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00010-of-00015.safetensors:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e93463a84347838d1754a5882d0f4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00011-of-00015.safetensors:   0%|          | 0.00/1.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b88cf7b61f4b499250b0f5de20f65d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00012-of-00015.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"881cdebaa9d340bfbed26c5b6bee6c7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00013-of-00015.safetensors:   0%|          | 0.00/1.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4feab2d18b4d4597844fcfb34327f26d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00014-of-00015.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1353ea4e36e4bdaa5702c2a57ff9ef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00015-of-00015.safetensors:   0%|          | 0.00/811M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7e5d88ece44fe9ae521292772748d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab9afcc3c61544d0aac8876b630fae3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/205 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36cf070222842dea4e2aa894748026a"}},"metadata":{}}]},{"cell_type":"markdown","source":"Test run with an attention image","metadata":{}},{"cell_type":"code","source":"get_image_description(\"/kaggle/input/utilss/new\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T04:17:05.344835Z","iopub.execute_input":"2024-07-12T04:17:05.345349Z","iopub.status.idle":"2024-07-12T04:18:15.848561Z","shell.execute_reply.started":"2024-07-12T04:17:05.345323Z","shell.execute_reply":"2024-07-12T04:18:15.847573Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"/kaggle/input/utilss/new\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:421: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference.\n  warnings.warn(\n2024-07-12 04:17:25.486726: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 04:17:25.486862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 04:17:25.604901: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"'This diagram represents a Transformer model architecture, which is a type of neural network architecture used in natural language processing (NLP). Transformers are particularly effective at handling sequence data.\\n\\nAt the bottom, the inputs are fed into the model, which are then processed through two separate encoding blocks: Positional Encoding and Input Embedding. Positional Encoding is a technique that adds information about the position of each word in the sequence to the word embeddings, allowing the model to understand the order of the words.\\n\\nInput Embedding converts the input sequence into a vector space representation, where each word is represented by a vector.\\n\\nAfter the embeddings are processed through the encoding blocks, the sequence is split into two halves (N/2 and N/2) and fed into the encoder part of the Transformer model.\\n\\nEach half goes through a series of identical layers, which are repeated N times (N is a hyperparameter that controls the depth of the model). These layers consist of three main components:\\n\\n1. Multi-Head Attention: This component allows the model to focus on different parts of the input sequence while ignoring others. It does this by breaking down the input sequence into multiple smaller sequences (heads) and then computing attention scores for each head.\\n\\n2. Masked Multi-Head Attention: This is a special version of Multi-Head Attention that is used for tasks like machine translation, where the input sequence may contain masked tokens (words that are not present in the target sequence). This component helps the model to learn to predict the masked tokens.\\n\\n3. Feed Forward: This component is a simple neural network that takes the input and applies a linear transformation followed by a non-linear activation function (ReLU).\\n\\nAfter each encoder layer, the outputs are added to the input sequence (residual connection) and then normalized (Add & Norm).\\n\\nFinally, the outputs of the encoder are fed into the decoder part of the Transformer model, which processes the sequence in the reverse order (shifted right). The decoder also consists of a series of identical layers with the same components as the encoder.\\n\\nThe outputs of the decoder are then processed through a series of linear transformations (Linear), which are then passed through a Softmax function to produce the final output probabilities.\\n\\nIn summary, the Transformer model processes the input sequence through a series of encoder and decoder layers, each containing Multi-Head Attention, Masked Multi-Head Attention, and Feed Forward components. The outputs of the decoder are then used to produce the final output probabilities. <|endoftext|>'"},"metadata":{}}]},{"cell_type":"markdown","source":"Retriever creation using Langchain parent retrieval","metadata":{}},{"cell_type":"code","source":"from langchain.retrievers import ParentDocumentRetriever, BM25Retriever, EnsembleRetriever\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.vectorstores import Chroma\nfrom langchain.storage import InMemoryStore\nimport pdfplumber\nimport os\n# from unstructured.partition.auto import partition\n\n# elements = partition(filename=\"/kaggle/input/utilss/a.pdf\")\n# main_text=\"\\n\\n\".join([str(el) for el in elements])","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:25:16.747320Z","iopub.execute_input":"2024-07-12T02:25:16.747839Z","iopub.status.idle":"2024-07-12T02:25:16.928944Z","shell.execute_reply.started":"2024-07-12T02:25:16.747807Z","shell.execute_reply":"2024-07-12T02:25:16.928191Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Customized doc class. Wanted to add more metadata like summary and title, some more context. Task for future.","metadata":{}},{"cell_type":"code","source":"class Document:\n    def __init__(self, content, metadata=None):\n        self.page_content = content\n        self.metadata = metadata if metadata is not None else {}\n\"\"\"\nA class is like a blueprint for creating objects. \nAn object is a collection of data (variables) and methods (functions) \nthat act on the data. For example, a Document class can be used \nto create Document objects, each with its own content and metadata.\n\"\"\"\n\n# Load selected document\n\ndef get_image_description(image_path):\n    print(image_path)\n    image_doc = Image.open(image_path).convert('RGB')\n#     image_doc = image\n    prompt = 'Analyze the graph if any or return the explanation of the image in 200 words'\n    inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"image\": image_doc, \"content\": prompt}],\n                                           add_generation_prompt=True, tokenize=True, return_tensors=\"pt\",\n                                           return_dict=True).to(\"cuda\")  # chat mode\n\n    gen_kwargs = {\"max_length\": 1024, \"do_sample\": True, \"top_k\": 1}\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, **gen_kwargs)\n        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n        response = tokenizer.decode(outputs[0])\n        \n    return response\n\n        \n\ndef start_image_processing(i,page,image):\n    images_in_page = page.images\n    page_height = page.height\n    image_bbox = (image['x0'] if image['x0'] else 0, page_height - image['y1'], image['x1'], page_height - image['y0'])\n    cropped_page = page.crop(image_bbox)\n    image_obj = cropped_page.to_image(resolution=400)\n    page_num = image['page_number']\n    image_path = f\"/kaggle/working/images_to_parse/{page_num}_{i}.png\"\n    image_obj.save(f\"/kaggle/working/images_to_parse/{image['page_number']}_{i}.png\")\n    description = get_image_description(image_path)\n    return description\n    \n\n\ndef load_single_document(file_path):\n    # Print the start of the document loading process\n    print(f\"Starting to load document from {file_path}\")\n    \n    if os.path.exists(file_path) and file_path.endswith(\".pdf\"):\n        # Print the filename of the PDF being processed\n        filename = os.path.basename(file_path)\n        print(f\"Loading PDF file: {filename}\")\n        \n        with pdfplumber.open(file_path) as pdf:\n            full_text = []\n            # Print the number of pages in the PDF\n            print(f\"Number of pages in PDF: {len(pdf.pages)}\")\n            \n            for page_number, page in enumerate(pdf.pages, start=1):\n#                 page_text=\"\"\n                page_text = page.extract_text()\n                \n                if page.images:\n#                     print(page_number)\n#                     description = \"\\n\"+start_image_processing(page)+\"\\n\"\n#                     print(description)\n#                     page_text+=description\n                    images_in_page = page.images\n                    page_height = page.height\n                    i=1\n                    for image in images_in_page:\n                        try:\n                            description = \"\\n\"+start_image_processing(i,page,image)+\"\\n\"\n                        except:\n                            description = \"\"\n                            print(\"couldnt parse image\")\n                        i+=1\n                        \n                        page_text+=description \n                    \n                if page_text:\n                    full_text.append(page_text)\n                    \n                # Print confirmation for each page processed\n                print(f\"Processed page {page_number}/{len(pdf.pages)}\")\n            \n            # Combine all pages' text into one string for the document\n            document_text = '\\n'.join(full_text)\n            # Create a Document object with the content and optional metadata\n            document = Document(content=document_text, metadata={'filename': filename})\n            # Print that the document has been successfully created\n            print(f\"Created document for {filename} with {len(full_text)} pages of text.\")\n    else:\n        print(f\"File not found or is not a PDF: {file_path}\")\n        return None\n    \n    return document","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:25:16.931162Z","iopub.execute_input":"2024-07-12T02:25:16.931507Z","iopub.status.idle":"2024-07-12T02:25:16.948776Z","shell.execute_reply.started":"2024-07-12T02:25:16.931475Z","shell.execute_reply":"2024-07-12T02:25:16.947830Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Loads each document in a different doc","metadata":{}},{"cell_type":"code","source":"# Specify the path to the PDF file\n!mkdir /kaggle/working/images_to_parse\ndocs=[]\nfile_path = \"/kaggle/input/utilss/\"\nfor x in os.listdir(file_path):\n#     if x.endswith('.pdf'):\n        print(f\"Loading doc {x}..............\")\n        document = load_single_document(file_path+x) # Load the document\n        docs.append(document)\n\n#save in pickle here","metadata":{"execution":{"iopub.status.busy":"2024-07-12T01:05:16.063783Z","iopub.execute_input":"2024-07-12T01:05:16.064400Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loading doc FMRI_vision_reconstruction.pdf..............\nStarting to load document from /kaggle/input/utilss/FMRI_vision_reconstruction.pdf\nLoading PDF file: FMRI_vision_reconstruction.pdf\nNumber of pages in PDF: 12\ncouldnt parse image\nProcessed page 1/12\ncouldnt parse image\n/kaggle/working/images_to_parse/2_2.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:421: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference.\n  warnings.warn(\n2024-07-12 01:05:38.894871: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 01:05:38.895008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 01:05:39.048386: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Processed page 2/12\ncouldnt parse image\nProcessed page 3/12\ncouldnt parse image\nProcessed page 4/12\ncouldnt parse image\nProcessed page 5/12\ncouldnt parse image\n/kaggle/working/images_to_parse/6_2.png\nProcessed page 6/12\ncouldnt parse image\nProcessed page 7/12\ncouldnt parse image\n/kaggle/working/images_to_parse/8_2.png\n/kaggle/working/images_to_parse/8_3.png\nProcessed page 8/12\ncouldnt parse image\nProcessed page 9/12\ncouldnt parse image\nProcessed page 10/12\ncouldnt parse image\nProcessed page 11/12\ncouldnt parse image\n/kaggle/working/images_to_parse/12_2.png\nProcessed page 12/12\nCreated document for FMRI_vision_reconstruction.pdf with 12 pages of text.\nLoading doc Vision_Instruction.pdf..............\nStarting to load document from /kaggle/input/utilss/Vision_Instruction.pdf\nLoading PDF file: Vision_Instruction.pdf\nNumber of pages in PDF: 25\nProcessed page 1/25\nProcessed page 2/25\n/kaggle/working/images_to_parse/3_1.png\nProcessed page 3/25\nProcessed page 4/25\nProcessed page 5/25\n/kaggle/working/images_to_parse/6_1.png\nProcessed page 6/25\nProcessed page 7/25\n/kaggle/working/images_to_parse/8_1.png\n/kaggle/working/images_to_parse/8_2.png\nProcessed page 8/25\nProcessed page 9/25\nProcessed page 10/25\nProcessed page 11/25\nProcessed page 12/25\nProcessed page 13/25\nProcessed page 14/25\n/kaggle/working/images_to_parse/15_1.png\nProcessed page 15/25\n/kaggle/working/images_to_parse/16_1.png\ncouldnt parse image\n/kaggle/working/images_to_parse/16_2.png\ncouldnt parse image\n/kaggle/working/images_to_parse/16_3.png\n/kaggle/working/images_to_parse/16_4.png\n/kaggle/working/images_to_parse/16_5.png\nProcessed page 16/25\n/kaggle/working/images_to_parse/17_1.png\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/input/utilss/docs.pkl', 'rb') as f:\n   docs = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:25:16.949885Z","iopub.execute_input":"2024-07-12T02:25:16.950155Z","iopub.status.idle":"2024-07-12T02:25:16.987849Z","shell.execute_reply.started":"2024-07-12T02:25:16.950132Z","shell.execute_reply":"2024-07-12T02:25:16.986988Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Make sure that the vectorstore is empty","metadata":{}},{"cell_type":"code","source":"coll = vectorstore.get()  # dict_keys(['ids', 'embeddings', 'documents', 'metadatas'])\n\nids_to_del = []\n\nfor idx in range(len(coll['ids'])):\n\n    id = coll['ids'][idx]\n    \n    metadata = coll['metadatas'][idx]\n    ids_to_del.append(id)\n\nvectorstore._collection.delete(ids_to_del)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:56:12.942367Z","iopub.execute_input":"2024-07-12T02:56:12.943103Z","iopub.status.idle":"2024-07-12T02:56:13.107305Z","shell.execute_reply.started":"2024-07-12T02:56:12.943070Z","shell.execute_reply":"2024-07-12T02:56:13.106403Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The main recursive splitter","metadata":{}},{"cell_type":"code","source":"# This text splitter is used to create the parent documents\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap = 100)\n\n# This text splitter is used to create the child documents\n# It should create documents smaller than the parent\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap = 80)\n\n# The vectorstore to use to index the child chunks\nvectorstore = Chroma(collection_name=\"return_split_parent_documents\", embedding_function=embeddings_model)\n\n# The storage layer for the parent documents\nstore = InMemoryStore()\n\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore, \n    docstore=store, \n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:56:22.373510Z","iopub.execute_input":"2024-07-12T02:56:22.373951Z","iopub.status.idle":"2024-07-12T02:56:22.387384Z","shell.execute_reply.started":"2024-07-12T02:56:22.373922Z","shell.execute_reply":"2024-07-12T02:56:22.386563Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Due to rate limit error of the api, sometimes the doc cannot be added once. In that case add elements in docs one by one","metadata":{}},{"cell_type":"code","source":"retriever.add_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:58:21.381973Z","iopub.execute_input":"2024-07-12T02:58:21.382664Z","iopub.status.idle":"2024-07-12T02:59:07.127289Z","shell.execute_reply.started":"2024-07-12T02:58:21.382633Z","shell.execute_reply":"2024-07-12T02:59:07.126288Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Some testing to see the results","metadata":{}},{"cell_type":"code","source":"# retriever.get_relevant_documents(\"what is multimodal conditioned in talking face generation\")\n# retriever.get_relevant_documents(\"what is generative adversial network?\")\n\n# vectorstore.similarity_search(\"what is multimodal conditioned in talking face generation\")\nretriever.get_relevant_documents(\"what is multimodal conditioned in talking face generation\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:25:36.533569Z","iopub.execute_input":"2024-07-12T03:25:36.533904Z","iopub.status.idle":"2024-07-12T03:25:37.144818Z","shell.execute_reply.started":"2024-07-12T03:25:36.533878Z","shell.execute_reply":"2024-07-12T03:25:37.143979Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'filename': 'Deepfake.pdf'}, page_content='lightweight deformation module explicitly decoupling sionsintalkingfacesratherthanfocusingsolelyonaudio\\nfacial pose and expression attributes. for facial expression synthesis. AMIGO [322] employs\\na sequence-to-sequence cross-modal emotion landmark\\n• Self-supervised Learning. Self-supervised learning\\ngeneration network to generate vivid landmarks aided\\nemploys supervisory signals inferred from the intrinsic\\nby audio information, ensuring that lips and emotions\\nstructure of the data, reducing the reliance on exter-\\nin the output image sequence are synchronized with the\\nnal data labels [207,253,321,328]. Oorloff et al. [207]\\ninput audio. However, existing methods still lack effec-\\nemploys self-supervised methods to train an encoder,\\ntive control over the intensity of emotions. In addition,\\ndisentangling identity and facial attribute information\\nTalkCLIP [178] introduces style parameters, expanding\\nof portrait images within the pre-defined latent space\\nthe style categories for text-guided talking video genera-\\nitself of a pre-trained StyleGAN2. Zhang et al. [328]\\ntion. Zhong et al. [348] propose a two-stage framework,\\nutilizes 3DMM to provide geometric guidance, employs\\nincorporating appearance priors during the generation\\npre-computed optical flow to guide motion field esti-\\nprocess to enhance the model’s ability to preserve at-\\nmation, and relies on pre-computed occlusion maps to\\ntributes of the target face. DR2 [326] explores practical\\nguide the perception and repair of occluded areas.\\nstrategies for reducing the training workload.\\n3.1.3 Talking Face Generation • Multimodal Conditioned. To generate more real-\\nistic talking videos, some methods [159,261,284,351]\\nIn this section, we review current methods from three introduce additional modal information on top of audio-\\nperspectives:audio/textdriven,multimodalconditioned, driven methods to guide facial pose and expression.'),\n Document(metadata={'filename': 'Deepfake.pdf'}, page_content='Targeted edits\\n(e) Comparison of Objects Operated by Different Tasks\\nfacial attributes, under the conditions of driving image, CVAE[239]introducesconditionalinput.VQ-VAE[255]\\nvideo, head pose, etc.. This technology often involves introducestheconceptofvectorquantizationtoimprove\\nthe support of facial motion capture technology, such the learning of latent representations. Subsequent mod-\\nas facial tracking or prediction based on deep learning els continually advance.\\nmodels [346]. 2) GANs [69] achieve high-quality generation through\\n• Talking Face Generation can be viewed as an adversarial training with an extra discriminator. Subse-\\nextension in time, aiming at generating a talking video quently, research on GANs experiences a surge, and\\nI ={Ii},i = 0,1,··· ,N −1 with the character in the currently, implementations based on GANs remain\\no o\\ntarget image I engaging in a conversation based on an the mainstream approach for various deepfake tasks.\\nt\\narbitrary driving source, such as text, audio, video, or CGAN [191] introduces conditional control variables to\\na multi-modal composite source. The lip movements, GANs.Pix2Pix[111]enhancesGANperformanceinspe-\\nfacial poses, expressions, emotions, and spoken content cific image translation tasks. StyleGAN [125] introduces\\ninformation of the character in the generated video the concept of style transfer to GANs. StyleGAN2 [126]\\nmatch the target information. further improves the quality and controllability of gen-\\nerated images. Additionally, some composite research\\n•Facial Attribute Editingaimstomodifytheseman-\\ncombines GANs with VAEs, e.g., CVAE-GAN [9].\\ntic information of the target face I (e.g., personality,\\nt\\nage, expressions, skin color, etc.) in a directed manner 3) Diffusion models [238] model the generation of data\\nas a diffusion process. DDPM [93] gains widespread\\nbased on individual interest and preference. Existing\\nattention for its outstanding generative performance,'),\n Document(metadata={'filename': 'Deepfake.pdf'}, page_content='diffusion-based, and 3D-model Technologies. We also GC-AVT[159]generatesrealistictalkingvideosbyinde-\\nsummarize them in Table 4. pendentlycontrollingheadpose,audioinformation,and\\n•Audio/TextDriven.Methodsaimtomapandguide facial expressions. This approach introduces an expres-\\nlip and facial movements in generated videos by under- sion source video, providing emotional information dur-\\nstanding the semantic information from the driving ing the speech and the pose source video. However, the\\nsource [230,259,332]. Early methods [32,58] perform video quality falls below expectations, and it struggles\\npoorly in terms of generalization and training complex- to handle complex background changes. Xu et al. [284]\\nity. After training, the models struggled to generalize to integrate text, image, and audio-emotional modalities\\nnew individuals, requiring extensive conversational data into a unified space to complement emotional content\\nfor training new characters. Researchers [33,217] pro- in textual information. Multimodal approaches have sig-\\npose their solutions from various perspectives. However, nificantly enhanced the vividness of generated videos,\\nMost of these methods prioritize generating lip move- but there is still room for exploration of organically\\nments aligned with semantic information, overlooking combining information driven by different sources and\\nessential aspects like identity and style, such as head modalities.\\nDeepfakeGenerationandDetection:ABenchmarkandSurvey 11\\nTable 4: Overview of representative talking face generation methods. Notations: ➊ LRW, ➋ VoxCeleb2, ➌ MEAD,\\n➍ Self-build, ➎ LRS2, ➏ HDTF, ➐ LRS3, ➑ CREMA-D, ➒ VoxCeleb, ➓ FFHQ.\\nMethod Venue Dataset Limitation Highlight\\nnevirD-txeT/oiduA')]"},"metadata":{}}]},{"cell_type":"markdown","source":"Query engine to streamline","metadata":{}},{"cell_type":"code","source":"def text_query_engine(query):\n    prompt_template = \"\"\"\"Using the information contained in the context,\n    give a comprehensive answer to the question with all the relevant details.\n    Respond only to the question asked, response should be relevant to the question.\n    If there are multiple questions, give answers to all questions in different points.\n    Context:{context}\n\n    Question: {question}\n    Answer:\"\"\"\n    PROMPT = PromptTemplate(\n        template=prompt_template, input_variables=[\"context\", \"question\"]\n    )\n\n    larger_chunk_relevant_docs =retriever.get_relevant_documents(query)[:3]\n\n    content = []\n    for cont in larger_chunk_relevant_docs:\n        content.append(cont.page_content)\n\n    response = llm.invoke(input=PROMPT.format_prompt(\n        context=content,\n        question=query\n    ).text).content\n    \n    return content,response\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:17:16.716666Z","iopub.execute_input":"2024-07-12T03:17:16.717015Z","iopub.status.idle":"2024-07-12T03:17:16.723411Z","shell.execute_reply.started":"2024-07-12T03:17:16.716987Z","shell.execute_reply":"2024-07-12T03:17:16.722467Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# content,response = text_query_engine(\"what are sine and cosine functions of different frequencies\")\ncontent,response = text_query_engine(\"what is multimodal conditioned in talking face generation\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:23:56.031456Z","iopub.execute_input":"2024-07-12T03:23:56.031827Z","iopub.status.idle":"2024-07-12T03:23:58.553193Z","shell.execute_reply.started":"2024-07-12T03:23:56.031799Z","shell.execute_reply":"2024-07-12T03:23:58.552452Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"response","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:23:58.554985Z","iopub.execute_input":"2024-07-12T03:23:58.555333Z","iopub.status.idle":"2024-07-12T03:23:58.560914Z","shell.execute_reply.started":"2024-07-12T03:23:58.555306Z","shell.execute_reply":"2024-07-12T03:23:58.559934Z"},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"\"Multimodal conditioned in talking face generation refers to the incorporation of additional modal information, such as text, image, audio-emotional modalities, and head pose, to guide facial pose and expression in order to generate more realistic talking videos. This approach aims to enhance the vividness of generated videos by combining information driven by different sources and modalities. Some representative methods that utilize multimodal conditioning in talking face generation include GC-AVT, which independently controls head pose, audio information, and facial expressions, and Xu et al.'s approach, which integrates text, image, and audio-emotional modalities into a unified space to complement emotional content in textual information.\""},"metadata":{}}]},{"cell_type":"markdown","source":"# Putting in csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:17:23.611485Z","iopub.execute_input":"2024-07-12T03:17:23.611839Z","iopub.status.idle":"2024-07-12T03:17:23.615900Z","shell.execute_reply.started":"2024-07-12T03:17:23.611815Z","shell.execute_reply":"2024-07-12T03:17:23.614984Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"df = pd.read_excel(\"/kaggle/input/utilss/Test_questions_IIT4th.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:17:24.476900Z","iopub.execute_input":"2024-07-12T03:17:24.477257Z","iopub.status.idle":"2024-07-12T03:17:24.499485Z","shell.execute_reply.started":"2024-07-12T03:17:24.477229Z","shell.execute_reply":"2024-07-12T03:17:24.498748Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"ques = df['Questions'].to_list()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:17:25.001263Z","iopub.execute_input":"2024-07-12T03:17:25.001993Z","iopub.status.idle":"2024-07-12T03:17:25.006192Z","shell.execute_reply.started":"2024-07-12T03:17:25.001963Z","shell.execute_reply":"2024-07-12T03:17:25.005224Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"ans[14]","metadata":{"execution":{"iopub.status.busy":"2024-07-12T04:31:52.281581Z","iopub.execute_input":"2024-07-12T04:31:52.282391Z","iopub.status.idle":"2024-07-12T04:31:52.288114Z","shell.execute_reply.started":"2024-07-12T04:31:52.282359Z","shell.execute_reply":"2024-07-12T04:31:52.287219Z"},"trusted":true},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"\"The paper suggests that future research directions for visual instruction tuning should focus on developing an end-to-end trained language-vision multimodal model for multiple tasks. It also emphasizes the need to explore methods for instruction-tuning large language models (LLMs) in the computer vision domain, similar to what has been done in the natural language processing (NLP) community. Additionally, the paper highlights the importance of studying the effectiveness of visual instruction tuning and its impact on the model's instruction-following abilities.\""},"metadata":{}}]},{"cell_type":"code","source":"cont = []\nans = []\nfor q in ques:\n    content,response = text_query_engine(q)\n    cont.append(content)\n    ans.append(response)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:17:27.521360Z","iopub.execute_input":"2024-07-12T03:17:27.522232Z","iopub.status.idle":"2024-07-12T03:18:20.130437Z","shell.execute_reply.started":"2024-07-12T03:17:27.522198Z","shell.execute_reply":"2024-07-12T03:18:20.129450Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"df['Answers'] = ans\ndf['Contexts'] = cont","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:18:20.132225Z","iopub.execute_input":"2024-07-12T03:18:20.132540Z","iopub.status.idle":"2024-07-12T03:18:20.137952Z","shell.execute_reply.started":"2024-07-12T03:18:20.132504Z","shell.execute_reply":"2024-07-12T03:18:20.137009Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:19:56.351331Z","iopub.execute_input":"2024-07-12T03:19:56.351713Z","iopub.status.idle":"2024-07-12T03:19:56.384269Z","shell.execute_reply.started":"2024-07-12T03:19:56.351685Z","shell.execute_reply":"2024-07-12T03:19:56.383191Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"                                            Questions  \\\n0   What models are mentioned in development timel...   \n1   What are mentioned for branch of prospects In ...   \n2   What are mentioned for branch of benchmark In ...   \n3   What is the Classical approach mentioned for f...   \n4   What is Strong imagination mentioned for fMRI ...   \n5   Which data is used for the first training step...   \n6   What techniques are used to fine-tune QWEN-CHA...   \n7   How does the performance of QWEN-14B compare t...   \n8   What is the score of Code Correctness Metric o...   \n9   What is results of pass@1 (%) on HumanEval of ...   \n10  What is the results of models on mathematical ...   \n11     What is the score of QWEN 7B on CommonsenseQA?   \n12  Can you describe the architecture of the Large...   \n13  What new state-of-the-art accuracy did LLaVA a...   \n14  What does the paper suggest as future research...   \n15  What was the accuracy of Human method for Natu...   \n16  How is Ichiran Ramen described in VISION_INSTR...   \n17  What is average performance score of MM-CoT La...   \n18  How much is the number of noun-phrase statics ...   \n19  How many languages are covered by the 680,000 ...   \n20  How do Whisper models compare to humans in ter...   \n21  What is the next step after “learned positiona...   \n22  What is average WordErrorRate of Whisper on Co...   \n23  How many hours of Japanese audio contained in ...   \n24  What is BLEU score of Whisper large-v2 on Fleu...   \n25  How much is the word error rate of Whisper med...   \n26  What is word error rate of Whisper tiny model ...   \n27  What is the primary goal of the Mind-to-Image ...   \n28  How does the Mind-to-Image pipeline differ fro...   \n29  What are the two modes of imagination studied ...   \n30  Please answer for multiple questions below.\\nW...   \n31  Please answer for multiple questions below.\\nH...   \n32  Please answer for multiple questions below.\\nW...   \n33  Please answer for multiple questions below.\\nH...   \n34  What future directions do the researchers sugg...   \n35  Please answer for multiple questions below.\\nW...   \n36  Please answer for multiple questions below.\\nW...   \n37  Please answer for multiple questions below.\\nH...   \n38  Please answer for multiple questions below.\\nC...   \n39  Please answer for multiple questions below.\\nW...   \n\n                                              Answers  \\\n0   The development timeline of three mainstream g...   \n1   The prospects mentioned in the time diagram th...   \n2   The four branches of benchmark mentioned in th...   \n3   The classical approach mentioned for fMRI data...   \n4   Strong imagination is mentioned as a data coll...   \n5   The data used for the first training step on t...   \n6   The techniques used to fine-tune QWEN-CHAT mod...   \n7   The performance of QWEN-14B is compared to oth...   \n8   The score of the Code Correctness Metric of QW...   \n9   The results of pass@1 (%) on HumanEval of CODE...   \n10  The results of models on mathematical reasonin...   \n11     The score of QWEN 7B on CommonsenseQA is 55.8.   \n12  The architecture of the Large Language and Vis...   \n13  LLaVA achieved a new state-of-the-art accuracy...   \n14  The paper suggests that future research direct...   \n15  The accuracy of the Human method for the Natur...   \n16  The given context does not contain any informa...   \n17  The average performance score of MM-CoT Large ...   \n18  The number of noun-phrase statistics after fil...   \n19  The 680,000 hours of audio data mentioned in t...   \n20  Whisper models have been found to be competiti...   \n21  The next step after \"learned positional encodi...   \n22  The average Word Error Rate (WER) of Whisper o...   \n23  The Multi lingual Speech Recognition data cont...   \n24  The BLEU score of Whisper large-v2 on Fleurs f...   \n25  The word error rate of the Whisper medium.en m...   \n26  The word error rate of the Whisper tiny model ...   \n27  The primary goal of the Mind-to-Image project ...   \n28  The Mind-to-Image pipeline differs from tradit...   \n29  The two modes of imagination studied in this r...   \n30  The main goal of visual instruction tuning in ...   \n31  1. LLaVA demonstrates its multimodal chat abil...   \n32  The main challenges in reconstructing images f...   \n33  The researchers evaluated the performance of t...   \n34  The researchers suggest that future directions...   \n35  1. QWEN is a series of large language models t...   \n36  1. The architecture used for the encoder-decod...   \n37  1. The Whisper model handles data pre-processi...   \n38  1. The concept of zero-shot transfer in relati...   \n39  1. The four mainstream research fields in deep...   \n\n                                             Contexts  \n0   [paper title and authors: Noname manuscript No...  \n1   [DeepfakeGenerationandDetection:ABenchmarkandS...  \n2   [DeepfakeGenerationandDetection:ABenchmarkandS...  \n3   [towardscreatingatechnologythatallowdirectreco...  \n4   [ThesestudiescollectivelyshowthepotentialoffMR...  \n5   [(V1,V2),associative(V3,V4,V5)andhigh-level(in...  \n6   [outperformslargermodelsinsomeinstances. Thefi...  \n7   [2023b),MPT(MosaicML,2023),Falcon(Almazroueiet...  \n8   [modelswhileengagingwithhumans(seeTable9).\\n13...  \n9   [7B - / 52.9 - / 55.6 - / 32.8 13.4 - / 35.8\\n...  \n10  [codegeneration,debugging,andinterpretation. T...  \n11  [34B 73.2 61.2\\n7B 37.2 35.8\\nQWEN-CHAT\\n14B 4...  \n12  [paper title and authors: Visual Instruction T...  \n13  [Subject ContextModality Grade\\nMethod Average...  \n14  [paper title and authors: Visual Instruction T...  \n15  [Subject ContextModality Grade\\nMethod Average...  \n16  [andcurationprocessinAppendix). Foreachimage,w...  \n17  [Subject ContextModality Grade\\nMethod Average...  \n18  [Table11: Thelistofinstructionsforbriefimagede...  \n19  [generatedbuttheoutputofexistingASRsystems. Re...  \n20  [muchbetteronotherdatasetsthanexpectedforitsLi...  \n21  [forthemultilingualmodelstoavoidexcessivefragm...  \n22  [Whispermodelsaretrainedon30-secondaudiochunks...  \n23  [generatedbuttheoutputofexistingASRsystems. Re...  \n24  [Latvian Maori\\nMacedonian\\nMalayalam Mongolia...  \n25  [Whispermodelsaretrainedon30-secondaudiochunks...  \n26  [Whisperbase.en 4.1 9.6 4.6 4.0 18.3 14.2 17.5...  \n27  [Step 3: Mind-to-Image model generates an imag...  \n28  [(V1,V2),associative(V3,V4,V5)andhigh-level(in...  \n29  [images(faceportraitsandnaturelandscapes)which...  \n30  [assistant. Inparticular,ourpapermakesthefollo...  \n31  [instructiontoanswerinanappropriatemanner.\\nQu...  \n32  [• Then,wetrainouradaptedfMRI-to-Imagemodelusi...  \n33  [ThesestudiescollectivelyshowthepotentialoffMR...  \n34  [images(faceportraitsandnaturelandscapes)which...  \n35  [etal.,2023;Google,2023;Anthropic,2023a;b). Th...  \n36  [RobustSpeechRecognitionviaLarge-ScaleWeakSupe...  \n37  [thankPamelaMishkinforadvisingtheprojectfromap...  \n38  [scribingthefirstorlastfewwordsofanaudiosegmen...  \n39  [this rapidly evolving field. First, we unify ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Questions</th>\n      <th>Answers</th>\n      <th>Contexts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What models are mentioned in development timel...</td>\n      <td>The development timeline of three mainstream g...</td>\n      <td>[paper title and authors: Noname manuscript No...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What are mentioned for branch of prospects In ...</td>\n      <td>The prospects mentioned in the time diagram th...</td>\n      <td>[DeepfakeGenerationandDetection:ABenchmarkandS...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What are mentioned for branch of benchmark In ...</td>\n      <td>The four branches of benchmark mentioned in th...</td>\n      <td>[DeepfakeGenerationandDetection:ABenchmarkandS...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the Classical approach mentioned for f...</td>\n      <td>The classical approach mentioned for fMRI data...</td>\n      <td>[towardscreatingatechnologythatallowdirectreco...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is Strong imagination mentioned for fMRI ...</td>\n      <td>Strong imagination is mentioned as a data coll...</td>\n      <td>[ThesestudiescollectivelyshowthepotentialoffMR...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Which data is used for the first training step...</td>\n      <td>The data used for the first training step on t...</td>\n      <td>[(V1,V2),associative(V3,V4,V5)andhigh-level(in...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>What techniques are used to fine-tune QWEN-CHA...</td>\n      <td>The techniques used to fine-tune QWEN-CHAT mod...</td>\n      <td>[outperformslargermodelsinsomeinstances. Thefi...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>How does the performance of QWEN-14B compare t...</td>\n      <td>The performance of QWEN-14B is compared to oth...</td>\n      <td>[2023b),MPT(MosaicML,2023),Falcon(Almazroueiet...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>What is the score of Code Correctness Metric o...</td>\n      <td>The score of the Code Correctness Metric of QW...</td>\n      <td>[modelswhileengagingwithhumans(seeTable9).\\n13...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>What is results of pass@1 (%) on HumanEval of ...</td>\n      <td>The results of pass@1 (%) on HumanEval of CODE...</td>\n      <td>[7B - / 52.9 - / 55.6 - / 32.8 13.4 - / 35.8\\n...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>What is the results of models on mathematical ...</td>\n      <td>The results of models on mathematical reasonin...</td>\n      <td>[codegeneration,debugging,andinterpretation. T...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>What is the score of QWEN 7B on CommonsenseQA?</td>\n      <td>The score of QWEN 7B on CommonsenseQA is 55.8.</td>\n      <td>[34B 73.2 61.2\\n7B 37.2 35.8\\nQWEN-CHAT\\n14B 4...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Can you describe the architecture of the Large...</td>\n      <td>The architecture of the Large Language and Vis...</td>\n      <td>[paper title and authors: Visual Instruction T...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>What new state-of-the-art accuracy did LLaVA a...</td>\n      <td>LLaVA achieved a new state-of-the-art accuracy...</td>\n      <td>[Subject ContextModality Grade\\nMethod Average...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>What does the paper suggest as future research...</td>\n      <td>The paper suggests that future research direct...</td>\n      <td>[paper title and authors: Visual Instruction T...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>What was the accuracy of Human method for Natu...</td>\n      <td>The accuracy of the Human method for the Natur...</td>\n      <td>[Subject ContextModality Grade\\nMethod Average...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>How is Ichiran Ramen described in VISION_INSTR...</td>\n      <td>The given context does not contain any informa...</td>\n      <td>[andcurationprocessinAppendix). Foreachimage,w...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>What is average performance score of MM-CoT La...</td>\n      <td>The average performance score of MM-CoT Large ...</td>\n      <td>[Subject ContextModality Grade\\nMethod Average...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>How much is the number of noun-phrase statics ...</td>\n      <td>The number of noun-phrase statistics after fil...</td>\n      <td>[Table11: Thelistofinstructionsforbriefimagede...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>How many languages are covered by the 680,000 ...</td>\n      <td>The 680,000 hours of audio data mentioned in t...</td>\n      <td>[generatedbuttheoutputofexistingASRsystems. Re...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>How do Whisper models compare to humans in ter...</td>\n      <td>Whisper models have been found to be competiti...</td>\n      <td>[muchbetteronotherdatasetsthanexpectedforitsLi...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>What is the next step after “learned positiona...</td>\n      <td>The next step after \"learned positional encodi...</td>\n      <td>[forthemultilingualmodelstoavoidexcessivefragm...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>What is average WordErrorRate of Whisper on Co...</td>\n      <td>The average Word Error Rate (WER) of Whisper o...</td>\n      <td>[Whispermodelsaretrainedon30-secondaudiochunks...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>How many hours of Japanese audio contained in ...</td>\n      <td>The Multi lingual Speech Recognition data cont...</td>\n      <td>[generatedbuttheoutputofexistingASRsystems. Re...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>What is BLEU score of Whisper large-v2 on Fleu...</td>\n      <td>The BLEU score of Whisper large-v2 on Fleurs f...</td>\n      <td>[Latvian Maori\\nMacedonian\\nMalayalam Mongolia...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>How much is the word error rate of Whisper med...</td>\n      <td>The word error rate of the Whisper medium.en m...</td>\n      <td>[Whispermodelsaretrainedon30-secondaudiochunks...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>What is word error rate of Whisper tiny model ...</td>\n      <td>The word error rate of the Whisper tiny model ...</td>\n      <td>[Whisperbase.en 4.1 9.6 4.6 4.0 18.3 14.2 17.5...</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>What is the primary goal of the Mind-to-Image ...</td>\n      <td>The primary goal of the Mind-to-Image project ...</td>\n      <td>[Step 3: Mind-to-Image model generates an imag...</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>How does the Mind-to-Image pipeline differ fro...</td>\n      <td>The Mind-to-Image pipeline differs from tradit...</td>\n      <td>[(V1,V2),associative(V3,V4,V5)andhigh-level(in...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>What are the two modes of imagination studied ...</td>\n      <td>The two modes of imagination studied in this r...</td>\n      <td>[images(faceportraitsandnaturelandscapes)which...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Please answer for multiple questions below.\\nW...</td>\n      <td>The main goal of visual instruction tuning in ...</td>\n      <td>[assistant. Inparticular,ourpapermakesthefollo...</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Please answer for multiple questions below.\\nH...</td>\n      <td>1. LLaVA demonstrates its multimodal chat abil...</td>\n      <td>[instructiontoanswerinanappropriatemanner.\\nQu...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Please answer for multiple questions below.\\nW...</td>\n      <td>The main challenges in reconstructing images f...</td>\n      <td>[• Then,wetrainouradaptedfMRI-to-Imagemodelusi...</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Please answer for multiple questions below.\\nH...</td>\n      <td>The researchers evaluated the performance of t...</td>\n      <td>[ThesestudiescollectivelyshowthepotentialoffMR...</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>What future directions do the researchers sugg...</td>\n      <td>The researchers suggest that future directions...</td>\n      <td>[images(faceportraitsandnaturelandscapes)which...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Please answer for multiple questions below.\\nW...</td>\n      <td>1. QWEN is a series of large language models t...</td>\n      <td>[etal.,2023;Google,2023;Anthropic,2023a;b). Th...</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Please answer for multiple questions below.\\nW...</td>\n      <td>1. The architecture used for the encoder-decod...</td>\n      <td>[RobustSpeechRecognitionviaLarge-ScaleWeakSupe...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Please answer for multiple questions below.\\nH...</td>\n      <td>1. The Whisper model handles data pre-processi...</td>\n      <td>[thankPamelaMishkinforadvisingtheprojectfromap...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Please answer for multiple questions below.\\nC...</td>\n      <td>1. The concept of zero-shot transfer in relati...</td>\n      <td>[scribingthefirstorlastfewwordsofanaudiosegmen...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Please answer for multiple questions below.\\nW...</td>\n      <td>1. The four mainstream research fields in deep...</td>\n      <td>[this rapidly evolving field. First, we unify ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.to_csv(\"Aditya.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:19:36.431543Z","iopub.execute_input":"2024-07-12T03:19:36.431887Z","iopub.status.idle":"2024-07-12T03:19:36.450436Z","shell.execute_reply.started":"2024-07-12T03:19:36.431860Z","shell.execute_reply":"2024-07-12T03:19:36.449553Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"df['Questions'][39]","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:18:20.163805Z","iopub.execute_input":"2024-07-12T03:18:20.164083Z","iopub.status.idle":"2024-07-12T03:18:20.169602Z","shell.execute_reply.started":"2024-07-12T03:18:20.164060Z","shell.execute_reply":"2024-07-12T03:18:20.168744Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"'Please answer for multiple questions below.\\nWhat are the four mainstream research fields in deepfake generation?\\nHow have diffusion models enhanced the generation capabilities of deepfakes?'"},"metadata":{}}]},{"cell_type":"code","source":"df['Answers'][38]","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:18:57.056514Z","iopub.execute_input":"2024-07-12T03:18:57.056890Z","iopub.status.idle":"2024-07-12T03:18:57.062590Z","shell.execute_reply.started":"2024-07-12T03:18:57.056862Z","shell.execute_reply":"2024-07-12T03:18:57.061641Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"'1. The concept of zero-shot transfer in relation to the Whisper models involves the ability of these models to perform well on languages and tasks for which they were not specifically trained. This means that the models can effectively transfer their learning from one language or task to another without the need for additional fine-tuning or specific training data.\\n\\n2. Concerns that might arise from scaling weakly supervised training approaches for speech recognition include the potential for reduced performance on lower-resource languages, as indicated by the poor speech recognition performance of Whisper on many languages. Additionally, there may be challenges in detecting and removing machine-generated transcripts from the training dataset, which can impact the overall performance of the models.'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Testing using RAGAs","metadata":{}},{"cell_type":"code","source":"!pip install ragas pymupdf","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:23:56.954500Z","iopub.execute_input":"2024-07-11T08:23:56.955375Z","iopub.status.idle":"2024-07-11T08:24:11.887600Z","shell.execute_reply.started":"2024-07-11T08:23:56.955342Z","shell.execute_reply":"2024-07-11T08:24:11.886355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom langchain_community.document_loaders.pdf import PyMuPDFLoader\nloader = PyMuPDFLoader('/kaggle/input/utilss/a.pdf')\ndocuments = loader.load()\n\n\n# generator with openai models\ngenerator_llm = llm\ncritic_llm = llm\nembeddings = embeddings_model\n\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm=generator_llm,\n    critic_llm=critic_llm,\n    embeddings=embeddings,\n)\n\ntestset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:24:11.889681Z","iopub.execute_input":"2024-07-11T08:24:11.890010Z","iopub.status.idle":"2024-07-11T08:27:37.868550Z","shell.execute_reply.started":"2024-07-11T08:24:11.889981Z","shell.execute_reply":"2024-07-11T08:27:37.867753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset.to_pandas()\nquestions = testset.to_pandas()[\"question\"].to_list()\nground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\nfrom ragas.metrics import (\n    answer_relevancy,\n    faithfulness,\n    context_recall,\n    context_precision,\n    answer_relevancy,\n    answer_similarity\n)\n\nfrom datasets import Dataset\n\nquestions = testset.to_pandas()[\"question\"].to_list()\nground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n\ndata = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n\nfor query in questions:\n    c,response = text_query_engine(query)\n    print(query)\n    data[\"question\"].append(query)\n    data[\"answer\"].append(response)\n    data[\"contexts\"].append(c)\n\n    \ndataset = Dataset.from_dict(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:28:13.228124Z","iopub.execute_input":"2024-07-11T08:28:13.228790Z","iopub.status.idle":"2024-07-11T08:28:26.448780Z","shell.execute_reply.started":"2024-07-11T08:28:13.228757Z","shell.execute_reply":"2024-07-11T08:28:26.447835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ragas import evaluate\n\nfrom ragas.metrics import (\n    answer_relevancy,\n    context_recall,\n    answer_correctness,\n    answer_similarity,\n    faithfulness,\n    context_precision,\n    context_relevancy,\n)\n\nresult = evaluate(\n    dataset = dataset,\n    llm = llm,\n    embeddings = embeddings,\n    metrics=[\n    answer_relevancy,\n    context_recall,\n    answer_correctness,\n    ],\n)\n\nresult.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:34:23.244772Z","iopub.execute_input":"2024-07-11T08:34:23.245697Z","iopub.status.idle":"2024-07-11T08:35:38.055537Z","shell.execute_reply.started":"2024-07-11T08:34:23.245663Z","shell.execute_reply":"2024-07-11T08:35:38.054640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.to_pandas().to_csv(\"/kaggle/working/output.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T06:18:52.878664Z","iopub.execute_input":"2024-07-11T06:18:52.879137Z","iopub.status.idle":"2024-07-11T06:18:52.900489Z","shell.execute_reply.started":"2024-07-11T06:18:52.879098Z","shell.execute_reply":"2024-07-11T06:18:52.899556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(result)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:35:56.009990Z","iopub.execute_input":"2024-07-11T08:35:56.010811Z","iopub.status.idle":"2024-07-11T08:35:56.015502Z","shell.execute_reply.started":"2024-07-11T08:35:56.010778Z","shell.execute_reply":"2024-07-11T08:35:56.014520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"usng both and parent chunk 1024: {'faithfulness': 0.7773, 'answer_relevancy': 0.8276, 'answer_correctness': 0.5453, 'answer_similarity': 0.6732}\n\nreduced parent chunk and no images: {'faithfulness': 0.9408, 'answer_relevancy': 0.8530, 'answer_correctness': 0.5791, 'answer_similarity': 0.7451, 'context_recall': 0.6833, 'context_relevancy': 0.0841}\n\n{'faithfulness': 0.9248, 'context_precision': 0.8000, 'context_recall': 0.6833, 'context_relevancy': 0.0632}\n\nfull pipeline: {'faithfulness': 0.7407, 'answer_relevancy': 0.7265, 'context_precision': 1.0000, 'answer_correctness': 0.6055, 'answer_similarity': 0.8713, 'context_recall': 0.9444, 'context_relevancy': 0.0383}\n\n# with images:\n# {'faithfulness': 0.9000,'answer_relevancy': 0.9118, 'context_precision': 1.0000, 'answer_correctness': 0.5037, 'answer_similarity': 0.7826,'context_recall': 0.9048, 'context_precision': 1.0000, 'context_relevancy': 0.0414}\n","metadata":{}},{"cell_type":"markdown","source":"home try:\n{'faithfulness': 0.7007, 'context_entity_recall': 0.0043, 'context_precision': 0.8750, 'context_relevancy': 0.0569}\n","metadata":{}},{"cell_type":"markdown","source":"# latest which changed chunks:\n# {'faithfulness': 0.8426, 'context_precision': 1.0000, 'context_relevancy': 0.0518,'answer_relevancy': 0.8721, 'context_recall': 0.8981, #'answer_correctness': 0.5442, 'answer_similarity': 0.8758}}\n","metadata":{}},{"cell_type":"markdown","source":"# latest with child chunk 400-> 500 and with unstructured:\n# {'faithfulness': 0.8056, 'context_precision': 1.0000, 'context_relevancy': 0.1004,answer_relevancy': 0.9330, 'context_recall': 0.7870, 'answer_correctness': 0.6952, 'answer_similarity': 0.8711}\n","metadata":{}},{"cell_type":"markdown","source":"# again with changed chunk:\n# {'faithfulness': 0.7969, 'context_precision': 0.8750, 'context_relevancy': 0.0293 ,'answer_relevancy': 0.8893, 'context_recall': 0.6875, 'answer_correctness': 0.5259, 'answer_similarity': 0.7645}\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}